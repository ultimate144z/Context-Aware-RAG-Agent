# Technical Documentation

[![Documentation](https://img.shields.io/badge/Docs-Complete-success.svg)](TECHNICAL_DOCUMENTATION.md)
[![Architecture](https://img.shields.io/badge/Architecture-RAG-blueviolet.svg)](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)

> Comprehensive technical reference for the Context-Aware RAG Agent implementation.

---

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Domain-Agnostic Design](#domain-agnostic-design)
3. [Retrieval Optimization](#retrieval-optimization)
4. [RAG Augmentation Process](#rag-augmentation-process)
5. [Usage Patterns & Configuration](#usage-patterns--configuration)
6. [Known Limitations](#known-limitations)

---

## Architecture Overview

### System Design

This RAG implementation prioritizes local execution, factual grounding, and domain flexibility. The architecture separates concerns across six primary stages, enabling independent optimization of each component.

### Pipeline Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     RAG PIPELINE                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  üìÑ PDF Input ‚Üí üîç Extract ‚Üí ‚úÇÔ∏è Chunk ‚Üí üß† Embed ‚Üí üíæ Store ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  üí¨ Query ‚Üí üîé Retrieve ‚Üí ü§ñ LLM ‚Üí Answer (with citations)‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **PDF Extraction** | pdfplumber, PyPDF2 | Extract selectable text |
| **OCR** | pytesseract, pdf2image | Handle scanned/image PDFs |
| **Text Processing** | Custom Python | Clean and merge text |
| **Chunking** | Sliding window (600 words) | Break into searchable pieces |
| **Embeddings** | sentence-transformers (all-MiniLM-L6-v2) | Convert text to 384-dim vectors |
| **Vector Store** | ChromaDB | Fast semantic search |
| **LLM** | Ollama (Mistral) | Local answer generation |
| **UI** | Streamlit | Web interface |

### Processing Workflow

#### Processing Phase (One-time per PDF)
```
1. Upload PDF ‚Üí data/raw_pdfs/
2. Extract text ‚Üí data/extracted_texts/ (JSON)
3. Chunk text ‚Üí data/processed_chunks/ (400-word chunks)
4. Embed chunks ‚Üí Convert to vectors (384 dimensions)
5. Store ‚Üí data/embeddings/ (ChromaDB)
```

#### Query Phase (Every question)
```
1. User asks question
2. Embed query ‚Üí 384-dim vector
3. Semantic search ‚Üí Find top-8 similar chunks (cosine similarity)
4. Filter by threshold (0.3 similarity minimum)
5. Format context ‚Üí "[Source 1: Page X]\n{chunk_text}\n---"
6. Build prompt ‚Üí Context + Question + Instructions
7. LLM generates answer ‚Üí Cites page numbers
8. Display ‚Üí Streamlit UI with sources
```

---

## Generic NLP Architecture

### Design Philosophy

**Challenge**: Early versions used hardcoded academic vocabulary (instructor, grading, attendance) that failed on business, medical, or legal documents.

**Solution**: Refactored to pattern-based transformations that work universally across domains.

### Pattern-Based Query Transformation

**Before (Hardcoded)**:
```python
synonyms = {
    'instructor': ['professor', 'teacher'],
    'grading': ['assessment', 'evaluation']
}
```

**After (Generic NLP)**:
```python
# Question reformulation patterns
"What is X?" ‚Üí ["X information", "X details", "X"]
"How to X?" ‚Üí ["X method", "X process", "X steps"]
"Where is X?" ‚Üí ["X location", "X address"]

# Preposition inversion
"email of instructor" ‚Üí "instructor email"
"price of product" ‚Üí "product price"

# Filler word removal
"Can you tell me X?" ‚Üí "X"
"Please explain Y" ‚Üí "Y"
```

### Universal Entity Recognition

**Universal Entity Patterns (Regex-based)**:
- Emails: `\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`
- URLs: `https?://[^\s]+`
- Phone numbers: `\+?\d[\d\s()-]{7,}\d`
- Dates: `\d{1,2}[/-]\d{1,2}[/-]\d{2,4}`
- Numbers with units: `\b\d+(\.\d+)?\s*(GB|MB|mg|kg|¬∞C|$)\b`
- Capitalized terms: Proper nouns, acronyms (CPU, RAM, FDA)
- Factual indicators: who, when, where, how many

### Cross-Domain Validation

Tested across 6 domains:
- **Academic**: instructor email, course policies
- **Technical**: CPU requirements, firewall setup
- **Business**: payment terms, refund policy
- **Medical**: dosage, side effects, contraindications
- **Legal**: liability clauses, termination terms
- **General**: document authors, product prices

**Validation Status**: 3-7 query variations generated per input using pattern-based rules (zero hardcoded domain vocabulary).

---

## Retrieval Engineering

### Optimization History

1. **Overly strict similarity threshold**: Initial 0.5 threshold filtered relevant factual chunks
2. **Insufficient context**: 5 chunks missed details spread across document sections
3. **Single-phrasing vulnerability**: Queries required exact wording to match chunk embeddings

### Implementation Strategy

#### 1. Threshold Calibration
```yaml
# config/settings.yaml
similarity_threshold: 0.3  # Was 0.5 (too strict!)
```
**Rationale**: Factual queries exhibit lower semantic similarity (0.3-0.4 range) despite high relevance. Lowering threshold improved recall on contact information, dates, and numerical data.

#### 2. Context Expansion
```yaml
# config/settings.yaml
top_k: 8  # Was 5
```
**Rationale**: Additional context helps LLM locate specific facts even when lower-ranked in similarity scores.

#### 3. Query Diversification
```python
# src/qa_pipeline/retriever.py
queries = expand_query_generic(query)  # Generates 3-7 variations
for variant in queries:
    results = chromadb.query(variant, top_k=top_k)
    all_results.merge(results)
```
**Rationale**: Different phrasings match different chunks. Result fusion improves recall by 30-50% over single-query baseline.

#### 4. Hybrid Scoring
```python
keywords = extract_keywords(query)  # ["email", "CPU", "deadline"]
for chunk in results:
    if keyword_match(chunk, keywords):
        adjusted_similarity = similarity * (1.0 + 0.15)  # 15% boost
```
**Rationale**: Semantic search alone misses exact-match importance. 15% boost balances semantic and lexical signals.

#### 5. Adaptive Thresholding
```python
if has_keyword_match(chunk, keywords):
    effective_threshold = max(0.2, threshold - 0.1)  # Lower threshold
else:
    effective_threshold = threshold  # Standard threshold
```
**Rationale**: Keyword presence indicates relevance even at lower cosine similarity. Adaptive threshold prevents false negatives.

### Performance Impact

| Query Type | Before | After |
|------------|--------|-------|
| Contact info (email, phone) | 0% | 80%+ |
| Grading/assessment details | 20% | 75%+ |
| Schedule/hours | 30% | 70%+ |
| Policy questions | 40% | 75%+ |
| General concepts | 70% | 75%+ |

### Multi-Document Handling

When user selects **2+ PDFs**, system distributes retrieval fairly:

```python
if len(filter_pdfs) > 1:
    per_pdf_k = max(1, top_k // len(filter_pdfs))  # Divide top_k
    for pdf in filter_pdfs:
        results = query_chromadb(pdf_filter=pdf, top_k=per_pdf_k)
        all_results.add(results)
```

**Example**: User selects 2 course PDFs and asks "What is the instructor name?"
- Retrieves 4 chunks from PDF A (8√∑2)
- Retrieves 4 chunks from PDF B (8√∑2)
- LLM sees both instructors ‚Üí Answer: "Course A: Dr. Smith, Course B: Prof. Johnson"

---

## RAG Augmentation Process

### Key Insight: Single Unified Prompt

**Common Misconception**: "LLM receives query and chunks separately"

**Reality**: Query + Retrieved chunks are **COMBINED into ONE text block** before sending to LLM.

### Step-by-Step Example

#### User Question:
```
"What is the leave policy?"
```

#### Step 1: Query Embedding
```python
query_vector = embedder.embed("What is the leave policy?")
# Result: [0.21, -0.43, 0.69, ..., 0.11] (384 dimensions)
```

#### Step 2: Semantic Search
```python
results = chromadb.query(query_vector, top_k=8)
# Top 3 matches:
# - Chunk #5 (Page 3): similarity = 0.89
# - Chunk #6 (Page 3): similarity = 0.78
# - Chunk #12 (Page 7): similarity = 0.72
```

#### Step 3: Context Formatting
```python
context = format_context(top_chunks)
```

**Output**:
```
[Source 1: Page 3]
Leave Policy Overview
Employees are entitled to annual leave based on their years of service:
- 1-3 years: 15 days annual leave
- 3-5 years: 20 days annual leave  
- 5+ years: 25 days annual leave
---

[Source 2: Page 3]
Emergency Leave
In cases of medical emergencies or family emergencies, employees may request 
emergency leave with less than 2 weeks notice. Documentation is required within 
3 business days.
---

[Source 3: Page 7]
Leave Carryover Policy
Unused annual leave can be carried over to the next calendar year with the 
following restrictions:
- Maximum 5 days can be carried over
- Carried over days must be used within Q1 of the following year
---
```

#### Step 4: Prompt Building (Augmentation)
```python
prompt = f"""Context from documents:
{context}

---

Based on the context above, please answer the following question. 
Cite the page numbers when possible.

Question: {query}

Answer:"""
```

**This SINGLE prompt** is what gets sent to the LLM!

#### Step 5: LLM Generation
```python
response = ollama.generate(prompt)
```

**Output**:
```
According to the company policy (Page 3), employees are entitled to annual leave 
based on their years of service:
- 1-3 years: 15 days
- 3-5 years: 20 days
- 5+ years: 25 days

Leave requests must be submitted at least 2 weeks in advance. Additionally, you 
can request emergency leave with less notice in case of medical or family 
emergencies (Page 3). You can carry over up to 5 days of unused leave to the 
next year, but they must be used in Q1 (Page 7).
```

### Why This Approach?

1. **LLMs accept single text input**: No separate channels for context vs query
2. **Explicit grounding**: Context appears BEFORE question ‚Üí "use THIS information"
3. **Citation support**: `[Source X: Page Y]` markers enable page references
4. **Token management**: Combined prompt fits within LLM's context window (8K tokens)

---

## Usage Patterns & Configuration

### Running Locally (Default)
This system is designed to run entirely on your machine:

```bash
streamlit run src/interface/app_streamlit.py
```

The Streamlit UI automatically handles:
- Multi-PDF upload and processing
- Real-time chunking, embedding, and vector storage
- Conversation memory across queries
- Citation tracking with page numbers

### Configurable Parameters

Edit `config/settings.yaml` to tune behavior:

```yaml
# Chunking
chunk_size: 600          # Words per chunk (increase for more context)
overlap: 75             # Word overlap between chunks (prevents context loss)
min_chunk_size: 150     # Minimum chunk size (prevents tiny fragments)

# Retrieval
top_k: 8                # Number of chunks to retrieve per query
similarity_threshold: 0.3  # Minimum similarity score (0.0-1.0)

# Processing
clean_whitespace: true  # Remove excessive whitespace
remove_page_numbers: true  # Strip PDF page number footers
```

### Using Different LLMs

Currently hardcoded to **Mistral via Ollama**. To swap LLMs:

1. Install alternative via Ollama: `ollama pull llama2` (or any model)
2. Update `config/model_config.json`:
```json
{
  "llm": {
    "model_name": "llama2",
    "temperature": 0.4,
    "max_tokens": 512
  }
}
```
3. Restart the app

### Extending the System

**Add custom retrieval logic:**
- Edit `src/qa_pipeline/retriever.py` - modify `retrieve_and_format()`
- Add your own keyword patterns, filtering, or ranking

**Add different vector stores:**
- Replace `ChromaManager` in `src/main.py`
- Implement same interface: `add_chunks()`, `query()`, `reset_collection()`

**Add new LLM providers:**
- Create `src/qa_pipeline/custom_llm.py`
- Implement same interface as `OllamaLLM`: `generate()`, `generate_with_retrieval()`

---

## Known Limitations

### 1. Conversation Memory Scope
- **Current**: Limited to last 3 Q&A pairs
- **Reason**: Prevents token overflow in LLM context window
- **Impact**: Very long conversations (10+ exchanges) may lose early context
- **Improvement**: Implement conversation summarization for longer sessions

### 2. Fixed Chunking Strategy
- **Current**: Word-count based (every 600 words with 75-word overlap)
- **Issue**: May occasionally split mid-sentence/paragraph
- **Mitigation**: 75-word overlap reduces context loss at boundaries
- **Improvement**: Semantic chunking (split at section boundaries)

### 3. OCR Quality
- **Current**: pytesseract (decent but not perfect)
- **Issue**: Complex layouts, tables, handwriting get garbled
- **Improvement**: Azure Computer Vision or AWS Textract (costs money)

### 4. No Multi-modal Support
- **Current**: Text only
- **Issue**: Images, charts, tables in PDFs are ignored
- **Improvement**: CLIP embeddings + GPT-4 Vision (requires GPU + API)

### 5. Single Language Model
- **Current**: Only Mistral via Ollama
- **Improvement**: Support multiple models (Llama, GPT-4, Claude) with dropdown

### 6. No Re-ranking
- **Current**: Top-k based solely on embedding similarity
- **Issue**: Embeddings miss keyword importance
- **Improvement**: Add cross-encoder re-ranking or BM25 hybrid

### 7. Performance on Large PDFs
- **Current**: Everything loads into memory
- **Issue**: 1000+ page PDFs could cause memory issues
- **Improvement**: Batch processing, lazy loading, streaming

---

## Conversation Memory Feature

### Overview
The system maintains conversational context by storing and using previous Q&A pairs. This enables natural follow-up questions without repeating context.

### How It Works

1. **Storage**: Streamlit session state stores all Q&A pairs with timestamps
2. **Context Limit**: Last 3 Q&A pairs are passed to LLM (prevents token overflow)
3. **Format**: 
   ```python
   conversation_history = [
       {"question": "What is the policy?", "answer": "The policy states..."},
       {"question": "What about exceptions?", "answer": "Exceptions include..."}
   ]
   ```
4. **Prompt Injection**: Context is prepended to current query in the prompt

### Example Usage

**Q1**: "What is the leave policy?"  
**A1**: "Employees get 15-20 days based on tenure (Page 3)."

**Q2**: "What about sick leave?" *(uses context from Q1/A1)*  
**A2**: "Sick leave is separate, 1 day per month up to 12 days/year (Page 3)."

**Q3**: "Can I carry over unused days?" *(uses context from Q1/A1 and Q2/A2)*  
**A3**: "Yes, maximum 5 days can roll over to next year (Page 7)."

### Benefits

**Natural Conversations**: Users can ask "What about X?" or "Tell me more"  
**Context Awareness**: LLM understands pronouns and references  
**Reduced Typing**: No need to repeat full context each time  
**Better UX**: Feels like talking to a human assistant  

### Implementation Details

- **API**: `ask_question(..., conversation_history=[...])` parameter
- **UI Indicator**: Shows "üí¨ Conversation Memory Active" when history exists
- **Clear History**: "üóëÔ∏è Clear History" button resets conversation
- **Automatic**: Works transparently - no user action required

---

## Configuration Reference

### settings.yaml
```yaml
chunk_size: 600          # Words per chunk (increased from 400)
overlap: 75              # Prevents context loss at boundaries (scaled up)
top_k: 8                 # Retrieve 8 chunks (was 5)
similarity_threshold: 0.3  # Minimum match quality (was 0.5)
ocr_lang: "eng"          # Tesseract language
```

### model_config.json
```json
{
  "llm": {
    "model_name": "mistral",
    "temperature": 0.4,     // Low = factual, High = creative
    "max_tokens": 512
  },
  "embedding_model": {
    "name": "all-MiniLM-L6-v2",
    "vector_dim": 384       // MUST match ChromaDB
  }
}
```

### paths_config.json
```json
{
  "data": {
    "raw_pdfs": "data/raw_pdfs",
    "embeddings": "data/embeddings"
  },
  "ollama": {
    "host": "http://localhost:11434"
  }
}
```

---

## Best Practices

### For Users

1. **PDF Quality**: Use high-quality scans (300+ DPI) for OCR accuracy
2. **Query Phrasing**: Ask specific questions ("What is the refund policy?" vs "Tell me about refunds")
3. **Multiple PDFs**: Select relevant documents only (not entire library)
4. **Verify Citations**: Check page numbers if answer seems off

### For Developers

1. **Test Incrementally**: Process 1 PDF first, then scale to multiple
2. **Monitor Logs**: Check `logs/` for debugging retrieval issues
3. **Tune Threshold**: Experiment with similarity threshold (0.2-0.5 range)
4. **Batch Embeddings**: Use `batch_size=8` for efficiency
5. **Clear Cache**: Delete `data/embeddings/` to reprocess with new settings

---

## Performance Metrics

### Processing Times (Typical)
- **10-page PDF**: ~15 seconds (5s extraction, 5s chunking, 5s embedding)
- **50-page PDF**: ~45 seconds
- **100-page PDF**: ~90 seconds

### Query Times
- **First query** (cold start): ~2-3 seconds
- **Subsequent queries** (cache hit): ~0.5-1 second
- **Speedup from caching**: ~95%

### Accuracy (After Optimization)
- **Factual queries**: 75-80% correct retrieval
- **Conceptual queries**: 75-80% correct retrieval
- **Overall improvement**: +40% from initial version

---

## Troubleshooting

### No results returned
- Check similarity threshold (try lowering to 0.2)
- Verify ChromaDB collection exists (`check_collection.py`)
- Check logs for query embedding errors

### Low-quality answers
- Increase top_k (try 10-12 chunks)
- Verify PDF extraction quality (check `data/extracted_texts/`)
- Try different prompt template (use `prompt_evaluator.py`)

### Slow performance
- Check Ollama is running (`ollama list`)
- Reduce chunk size (400 ‚Üí 300 words)
- Ensure embeddings are cached (don't re-process PDFs)

### OCR errors
- Verify Tesseract installed (`tesseract --version`)
- Check Poppler installed (required for pdf2image)
- Increase image DPI in `ocr_extractor.py`

---

## Additional Resources

- **ChromaDB Docs**: https://docs.trychroma.com/
- **Sentence Transformers**: https://www.sbert.net/
- **Ollama Models**: https://ollama.ai/library
- **Streamlit Docs**: https://docs.streamlit.io/

---

**Document Version**: 1.0  
**Last Updated**: November 11, 2025  
**Maintained By**: Sarim Farooq
