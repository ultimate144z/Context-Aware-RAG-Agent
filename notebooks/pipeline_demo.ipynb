{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b17323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b3f96",
   "metadata": {},
   "source": [
    "## 1. Initialize Pipeline\n",
    "\n",
    "Load the RAG pipeline with default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import RAGPipeline\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Setup logging\n",
    "setup_logger(\"Pipeline_Demo\", log_dir=\"../logs\", log_to_file=False)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = RAGPipeline(config_dir=\"../config\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG Pipeline Initialized\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Chunk size: {pipeline.settings['chunk_size']} words\")\n",
    "print(f\"Overlap: {pipeline.settings['overlap']} words\")\n",
    "print(f\"Top-K retrieval: {pipeline.settings['top_k']}\")\n",
    "print(f\"Similarity threshold: {pipeline.settings['similarity_threshold']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dbdd6",
   "metadata": {},
   "source": [
    "## 2. Check Database Status\n",
    "\n",
    "View current vector database statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database stats\n",
    "stats = pipeline.get_database_stats()\n",
    "\n",
    "print(\"\\nDatabase Statistics:\")\n",
    "print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"Unique PDFs: {stats['num_pdfs']}\")\n",
    "\n",
    "if stats['pdfs']:\n",
    "    print(\"\\nProcessed PDFs:\")\n",
    "    for pdf_name, count in stats['pdfs'].items():\n",
    "        print(f\"  - {pdf_name}: {count} chunks\")\n",
    "else:\n",
    "    print(\"\\nNo PDFs processed yet. Upload a PDF to data/raw_pdfs/ and process it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b0f72",
   "metadata": {},
   "source": [
    "## 3. Process a PDF (Optional)\n",
    "\n",
    "If you have a PDF in `data/raw_pdfs/`, uncomment and run this cell to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to process a PDF\n",
    "# pdf_path = \"../data/raw_pdfs/your_document.pdf\"\n",
    "# \n",
    "# if os.path.exists(pdf_path):\n",
    "#     print(f\"Processing: {pdf_path}\")\n",
    "#     success = pipeline.process_pdf(pdf_path)\n",
    "#     \n",
    "#     if success:\n",
    "#         print(\"\\nPDF processed successfully!\")\n",
    "#         # Refresh stats\n",
    "#         stats = pipeline.get_database_stats()\n",
    "#         print(f\"Updated total chunks: {stats['total_chunks']}\")\n",
    "#     else:\n",
    "#         print(\"\\nPDF processing failed. Check logs for details.\")\n",
    "# else:\n",
    "#     print(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "print(\"Skipping PDF processing (uncomment to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12521a",
   "metadata": {},
   "source": [
    "## 4. Test Retrieval\n",
    "\n",
    "Query the vector database to see what chunks are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c45b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "\n",
    "# Check if database has chunks\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    # Retrieve relevant chunks\n",
    "    retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "    \n",
    "    print(f\"Retrieved {retriever_result['num_chunks']} chunks:\\n\")\n",
    "    \n",
    "    for i, chunk in enumerate(retriever_result['chunks'][:3], 1):  # Show top 3\n",
    "        print(f\"[Chunk {i}]\")\n",
    "        print(f\"PDF: {chunk['pdf_name']}\")\n",
    "        print(f\"Page: {chunk['page_number']}\")\n",
    "        print(f\"Similarity: {chunk['similarity']:.3f}\")\n",
    "        print(f\"Text preview: {chunk['text'][:200]}...\\n\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa21be",
   "metadata": {},
   "source": [
    "## 5. Generate Answer\n",
    "\n",
    "Use the LLM to generate an answer based on retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if database has chunks\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    # Ask question\n",
    "    result = pipeline.ask_question(test_query, show_sources=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANSWER\")\n",
    "    print(\"=\"*60)\n",
    "    print(result['answer'])\n",
    "    \n",
    "    if result['sources']:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SOURCES\")\n",
    "        print(\"=\"*60)\n",
    "        for source in result['sources']:\n",
    "            print(f\"\\n[{source['pdf_name']} - Page {source['page_number']}]\")\n",
    "            print(f\"Similarity: {source['similarity']:.3f}\")\n",
    "            print(f\"Text: {source['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1535a7",
   "metadata": {},
   "source": [
    "## 6. Test Conversation Memory\n",
    "\n",
    "Demonstrate how conversation history affects answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06514b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    conversation_history = []\n",
    "    \n",
    "    # Question 1\n",
    "    q1 = \"What is this document about?\"\n",
    "    print(f\"Q1: {q1}\")\n",
    "    result1 = pipeline.ask_question(q1)\n",
    "    print(f\"A1: {result1['answer'][:200]}...\\n\")\n",
    "    \n",
    "    # Store in history\n",
    "    conversation_history.append({\n",
    "        \"question\": q1,\n",
    "        \"answer\": result1['answer']\n",
    "    })\n",
    "    \n",
    "    # Question 2 (with context)\n",
    "    q2 = \"Tell me more about that\"\n",
    "    print(f\"Q2: {q2}\")\n",
    "    result2 = pipeline.ask_question(q2, conversation_history=conversation_history)\n",
    "    print(f\"A2: {result2['answer'][:200]}...\\n\")\n",
    "    \n",
    "    print(\"\\nConversation memory allows follow-up questions to reference previous context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0483b67",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Measure retrieval and generation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    queries = [\n",
    "        \"What is the main topic?\",\n",
    "        \"Who is the author?\",\n",
    "        \"What are the key findings?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Performance Benchmark:\\n\")\n",
    "    \n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        result = pipeline.ask_question(query)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Time: {elapsed:.2f}s\")\n",
    "        print(f\"Answer length: {len(result['answer'])} chars\")\n",
    "        print(f\"Sources: {len(result.get('sources', []))}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d11a6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Pipeline initialization and configuration\n",
    "- Database statistics and management\n",
    "- Semantic retrieval testing\n",
    "- Answer generation with citations\n",
    "- Conversation memory functionality\n",
    "- Performance benchmarking\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different queries to test retrieval quality\n",
    "- Adjust `settings.yaml` parameters and observe effects\n",
    "- Use `prompt_testing.ipynb` to experiment with prompt engineering"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
