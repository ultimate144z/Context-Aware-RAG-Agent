{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bade9",
   "metadata": {},
   "source": [
    "## 1. Initialize Components\n",
    "\n",
    "Load the necessary components for prompt testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import RAGPipeline\n",
    "from src.qa_pipeline.ollama_llm import OllamaLLM\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Setup logging (console only for cleaner output)\n",
    "setup_logger(\"Prompt_Testing\", log_dir=\"../logs\", log_to_file=False)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = RAGPipeline(config_dir=\"../config\")\n",
    "\n",
    "# Check database\n",
    "stats = pipeline.get_database_stats()\n",
    "print(f\"Database contains {stats['total_chunks']} chunks from {stats['num_pdfs']} PDFs\")\n",
    "\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"\\nWarning: No chunks in database. Process a PDF first using pipeline_demo.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6deae0",
   "metadata": {},
   "source": [
    "## 2. Define Prompt Templates\n",
    "\n",
    "Create different prompt templates to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7179550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates to test\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"default\": \"\"\"\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Based on the context above, please answer the following question. \n",
    "Cite the page numbers when possible.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"detailed\": \"\"\"\n",
    "You are a helpful assistant that answers questions based on provided document excerpts.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "- Answer the question using ONLY the information from the context above\n",
    "- Include specific page numbers in your citations (e.g., \"Page 3\")\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Be concise but complete\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"concise\": \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a concise answer with page citations:\"\"\",\n",
    "    \n",
    "    \"structured\": \"\"\"\n",
    "Document Context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please structure your answer as follows:\n",
    "1. Direct answer (1-2 sentences)\n",
    "2. Supporting details from the documents\n",
    "3. Page references\n",
    "\n",
    "Answer:\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Loaded prompt templates:\")\n",
    "for name in PROMPT_TEMPLATES.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749cfc62",
   "metadata": {},
   "source": [
    "## 3. Test a Single Prompt\n",
    "\n",
    "Test one prompt template with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297209d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "template_name = \"default\"  # Change to test different templates\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    print(f\"Testing prompt template: {template_name}\")\n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    \n",
    "    # Retrieve context\n",
    "    retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "    context = retriever_result['formatted_context']\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt_template = PROMPT_TEMPLATES[template_name]\n",
    "    prompt = prompt_template.format(context=context, question=test_query)\n",
    "    \n",
    "    print(\"Generated Prompt:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(prompt)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = pipeline.llm.generate(prompt)\n",
    "    \n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(answer)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72692b",
   "metadata": {},
   "source": [
    "## 4. Compare Multiple Prompts\n",
    "\n",
    "Run the same query through different prompt templates and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_query = \"What are the key findings?\"\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    \n",
    "    # Retrieve context once (same for all templates)\n",
    "    retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "    context = retriever_result['formatted_context']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test each template\n",
    "    for name, template in PROMPT_TEMPLATES.items():\n",
    "        print(f\"Testing template: {name}\")\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = template.format(context=context, question=test_query)\n",
    "        \n",
    "        # Generate answer with timing\n",
    "        start_time = time.time()\n",
    "        answer = pipeline.llm.generate(prompt)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results[name] = {\n",
    "            \"answer\": answer,\n",
    "            \"time\": elapsed,\n",
    "            \"length\": len(answer),\n",
    "            \"has_page_ref\": \"page\" in answer.lower()\n",
    "        }\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n[{name.upper()}]\")\n",
    "        print(f\"Time: {result['time']:.2f}s\")\n",
    "        print(f\"Length: {result['length']} chars\")\n",
    "        print(f\"Has page reference: {result['has_page_ref']}\")\n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3901f3",
   "metadata": {},
   "source": [
    "## 5. Citation Analysis\n",
    "\n",
    "Check if answers correctly cite page numbers from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_page_numbers(text):\n",
    "    \"\"\"Extract page numbers mentioned in text.\"\"\"\n",
    "    # Match patterns like \"Page 3\", \"page 5\", \"p. 7\"\n",
    "    pattern = r'[Pp]age\\s+(\\d+)|[Pp]\\.\\s*(\\d+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    # Flatten and filter empty strings\n",
    "    pages = [int(p) for match in matches for p in match if p]\n",
    "    return sorted(set(pages))\n",
    "\n",
    "test_query = \"What is discussed in this document?\"\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    # Get retrieval result\n",
    "    retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "    \n",
    "    # Extract actual page numbers from retrieved chunks\n",
    "    actual_pages = [chunk['page_number'] for chunk in retriever_result['chunks']]\n",
    "    actual_pages = sorted(set(actual_pages))\n",
    "    \n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(f\"Retrieved chunks from pages: {actual_pages}\\n\")\n",
    "    \n",
    "    # Test with default prompt\n",
    "    result = pipeline.ask_question(test_query)\n",
    "    answer = result['answer']\n",
    "    \n",
    "    # Extract cited pages\n",
    "    cited_pages = extract_page_numbers(answer)\n",
    "    \n",
    "    print(\"Answer:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    \n",
    "    print(f\"\\nCitation Analysis:\")\n",
    "    print(f\"Pages in retrieved context: {actual_pages}\")\n",
    "    print(f\"Pages cited in answer: {cited_pages}\")\n",
    "    \n",
    "    # Check accuracy\n",
    "    correct_citations = [p for p in cited_pages if p in actual_pages]\n",
    "    incorrect_citations = [p for p in cited_pages if p not in actual_pages]\n",
    "    \n",
    "    print(f\"\\nCorrect citations: {correct_citations}\")\n",
    "    print(f\"Incorrect citations: {incorrect_citations}\")\n",
    "    \n",
    "    if cited_pages:\n",
    "        accuracy = len(correct_citations) / len(cited_pages) * 100\n",
    "        print(f\"Citation accuracy: {accuracy:.1f}%\")\n",
    "    else:\n",
    "        print(\"No citations found in answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42c7db",
   "metadata": {},
   "source": [
    "## 6. Prompt Length Analysis\n",
    "\n",
    "Analyze how prompt length affects answer quality and generation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f97e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_query = \"Summarize the main points\"\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    # Test with different top_k values (affects context length)\n",
    "    top_k_values = [3, 5, 8]\n",
    "    \n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    print(\"Testing different context sizes:\\n\")\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        # Retrieve with different top_k\n",
    "        pipeline.retriever.top_k = k\n",
    "        retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "        context = retriever_result['formatted_context']\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = PROMPT_TEMPLATES['default'].format(context=context, question=test_query)\n",
    "        \n",
    "        # Measure generation time\n",
    "        start_time = time.time()\n",
    "        answer = pipeline.llm.generate(prompt)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate sizes\n",
    "        prompt_tokens = len(prompt.split())  # Rough estimate\n",
    "        answer_tokens = len(answer.split())\n",
    "        \n",
    "        print(f\"[Top-K = {k}]\")\n",
    "        print(f\"Prompt size: ~{prompt_tokens} tokens\")\n",
    "        print(f\"Answer size: ~{answer_tokens} tokens\")\n",
    "        print(f\"Generation time: {elapsed:.2f}s\")\n",
    "        print(f\"Answer preview: {answer[:150]}...\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Reset to default\n",
    "    pipeline.retriever.top_k = pipeline.settings['top_k']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682690d8",
   "metadata": {},
   "source": [
    "## 7. Custom Prompt Testing\n",
    "\n",
    "Test your own custom prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom prompt here\n",
    "CUSTOM_PROMPT = \"\"\"\n",
    "You are analyzing a document. Here is relevant information:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide:\n",
    "- A direct answer\n",
    "- Key supporting points\n",
    "- Page citations in [Page X] format\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "test_query = \"What are the recommendations?\"\n",
    "\n",
    "stats = pipeline.get_database_stats()\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"No chunks in database. Process a PDF first.\")\n",
    "else:\n",
    "    print(f\"Query: {test_query}\\n\")\n",
    "    \n",
    "    # Retrieve context\n",
    "    retriever_result = pipeline.retriever.retrieve_and_format(test_query)\n",
    "    context = retriever_result['formatted_context']\n",
    "    \n",
    "    # Build custom prompt\n",
    "    prompt = CUSTOM_PROMPT.format(context=context, question=test_query)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = pipeline.llm.generate(prompt)\n",
    "    \n",
    "    print(\"Answer with custom prompt:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(answer)\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze\n",
    "    cited_pages = extract_page_numbers(answer)\n",
    "    print(f\"\\nPages cited: {cited_pages}\")\n",
    "    print(f\"Answer length: {len(answer)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2497e47",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Testing multiple prompt templates\n",
    "- Comparing answer quality and generation time\n",
    "- Analyzing citation accuracy\n",
    "- Measuring impact of context length\n",
    "- Custom prompt experimentation\n",
    "\n",
    "**Key Findings**:\n",
    "- Different prompts can significantly affect answer structure and citation quality\n",
    "- Longer context (higher top_k) may improve answer completeness but increases latency\n",
    "- Explicit instructions for citations improve page reference accuracy\n",
    "\n",
    "**Recommendations**:\n",
    "1. Use structured prompts for better answer organization\n",
    "2. Explicitly request page citations in the prompt\n",
    "3. Balance context length vs. generation speed based on use case\n",
    "4. Test prompts with various query types (factual, conceptual, comparison)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
